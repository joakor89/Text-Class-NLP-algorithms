{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyAhmlbo24VQ"
      },
      "source": [
        "import math\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSHovwBR1ZfA"
      },
      "source": [
        "## Preparación del corpus de emails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8P_ve0L0Gyu"
      },
      "source": [
        "!git clone https://github.com/pachocamacho1990/datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFrqDzfu04pJ"
      },
      "source": [
        "! unzip datasets/email/plaintext/corpus1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7BKMjaR28jW"
      },
      "source": [
        "os.listdir('corpus1/spam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJTtSZto1jC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8550d8e7-0273-4053-e452-80216f463658"
      },
      "source": [
        "# Setting Environment\n",
        "\n",
        "data = []\n",
        "classes = []\n",
        "# Reading Spam Data\n",
        "for file in os.listdir('corpus1/spam'):\n",
        "  with open('corpus1/spam/'+file, encoding='latin-1') as f:\n",
        "    data.append(f.read())\n",
        "    classes.append('spam')\n",
        "# Reading Ham Data \n",
        "for file in os.listdir('corpus1/ham'):\n",
        "  with open('corpus1/ham/'+file, encoding='latin-1') as f:\n",
        "    data.append(f.read())\n",
        "    classes.append('ham')\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5172"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NOb61Ik7MI-"
      },
      "source": [
        "## Building Naive Bayes Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXXlUjrJ7DjR"
      },
      "source": [
        "## Spacy Tokenizer\n",
        "\n",
        "* Documentation: https://spacy.io/api/tokenizer\n",
        "* ¿How The Tokenizer Works? https://spacy.io/usage/linguistic-features#how-tokenizer-works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePmKCQpw4cqK"
      },
      "source": [
        "#@title Spacy Tokenizer Libraries\n",
        "from spacy.tokenizer import Tokenizer\n",
        "# English Vocabulary\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# English - Natural Language Processing \n",
        "nlp = English()\n",
        "# Tokenizer Instance Set-Up\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF1cGmZ293OA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773d4d89-7418-411a-d8dc-662f3acd52e9"
      },
      "source": [
        "print([t.text for t in tokenizer(data[0])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Subject:', 'confidence', 'is', 'back', '\\n', 'hello', ',', '\\n', 'my', 'boyfriend', 'began', 'having', 'problems', 'with', 'erections', '(', 'he', \"'\", 's', 'older', ')', '\\n', 'and', 'i', 'suggested', 'he', 'look', 'into', 'vlagrra', 'softtabs', '.', '\\n', 'boy', ',', 'am', 'i', 'glad', 'he', 'did', '!', '\\n', 'the', 'first', 'time', 'he', 'tried', 'it', ',', 'one', '50', 'mg', 'piil', 'did', 'nothing', 'so', 'he', 'took', 'another', 'and', 'that', 'was', 'a', 'mistake', '.', 'three', 'hours', 'later', 'he', 'was', 'still', 'rock', 'hard', 'and', 'had', 'come', 'multiple', 'times', '(', 'so', 'had', 'i', ')', '!', '!', 'since', 'then', 'a', 'single', '50', 'mg', 'dose', 'does', 'ithe', 'first', 'of', 'these', 'was', 'that', 'he', 'should', 'have', 'been', 'brought', 'to', 'trial', 'att', 'very', 'well', '-', '-', 'he', \"'\", 's', 'now', 'good', 'for', 'almost', '2', 'hours', 'of', 'good', 'hard', 'sex', 'that', 'leaves', 'both', 'of', 'us', 'worn', 'out', '.', '\\n', '-', 'bobbie', ',', '21', 'female', 'usa', '\\n', 'try', 'it', 'with', 'pharmacybymail', 'shop', '.', '\\n', 'http', ':', '/', '/', 'tabboulehs', '.', 'net', '/', 'cs', '/', '?', 'aa', '\\n', 'rockky', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4i6IQGx7Tul"
      },
      "source": [
        "### Algorithm Main class\n",
        "\n",
        "Remember the most probable class is given by (in logarithmic computation space):\n",
        "\n",
        "\n",
        "$$\\hat{c} = {\\arg \\max}_{(c)}\\log{P(c)}\n",
        " +\\sum_{i=1}^n\n",
        "\\log{ P(f_i \\vert c)}\n",
        "$$\n",
        "\n",
        "To avoid outliers, we will use Laplace smoothing like so:\n",
        "\n",
        "$$\n",
        "P(f_i \\vert c) = \\frac{C(f_i, c)+1}{C(c) + \\vert V \\vert}\n",
        "$$\n",
        "\n",
        "being $\\vert V \\vert$ the vocabulary length of our training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTgbTusfyPHW"
      },
      "source": [
        "# Numerical Manipulation\n",
        "import numpy as np\n",
        "\n",
        "# Naive Bayes Classifier\n",
        "class NaiveBayesClassifier():\n",
        "  nlp = English()\n",
        "  tokenizer = Tokenizer(nlp.vocab)\n",
        "  \n",
        "  def tokenize(self, doc):\n",
        "    return  [t.text.lower() for t in tokenizer(doc)]\n",
        "\n",
        "  def word_counts(self, words):\n",
        "    wordCount = {}\n",
        "    for w in words: \n",
        "      if w in wordCount.keys():\n",
        "        wordCount[w] += 1\n",
        "      else:\n",
        "        wordCount[w] = 1\n",
        "    return wordCount\n",
        "\n",
        "  def fit(self, data, classes):\n",
        "    n = len(data)\n",
        "    self.unique_classes = set(classes)\n",
        "    self.vocab = set()\n",
        "    self.classCount = {} #C(c)\n",
        "    self.log_classPriorProb = {} #P(c)\n",
        "    self.wordConditionalCounts = {} #C(w|c)\n",
        "    # Class Counter \n",
        "    for c in classes:\n",
        "      if c in self.classCount.keys():\n",
        "        self.classCount[c] += 1\n",
        "      else:\n",
        "        self.classCount[c] = 1\n",
        "    # P(c) Calculation\n",
        "    for c in self.classCount.keys():\n",
        "      self.log_classPriorProb[c] = math.log(self.classCount[c]/n)\n",
        "      self.wordConditionalCounts[c] = {}\n",
        "    # C(w|c) Calculation\n",
        "    for text, c in zip(data, classes):\n",
        "      counts = self.word_counts(self.tokenize(text))\n",
        "      for word, count in counts.items():\n",
        "        if word not in self.vocab:\n",
        "          self.vocab.add(word)\n",
        "        if word not in self.wordConditionalCounts[c]:\n",
        "          self.wordConditionalCounts[c][word] = 0.0\n",
        "        self.wordConditionalCounts[c][word] += count\n",
        "\n",
        "  def predict(self, data):\n",
        "    results = []\n",
        "    for text in data:\n",
        "      words = set(self.tokenize(text))\n",
        "      scoreProb = {}\n",
        "      for word in words: \n",
        "        if word not in self.vocab: continue # Ignoring New Words \n",
        "        # P(w|c) Laplacian Smoother\n",
        "        for c in self.unique_classes:\n",
        "          log_wordClassProb = math.log(\n",
        "              (self.wordConditionalCounts[c].get(word, 0.0)+1)/(self.classCount[c]+len(self.vocab)))\n",
        "          scoreProb[c] = scoreProb.get(c, self.log_classPriorProb[c]) + log_wordClassProb\n",
        "      arg_maxprob = np.argmax(np.array(list(scoreProb.values())))\n",
        "      results.append(list(scoreProb.keys())[arg_maxprob])\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE9Aus71bdUx"
      },
      "source": [
        "### Scikit Learn Utilities\n",
        "* `train_test_split`: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "* `accuracy_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
        "\n",
        "* `precision_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
        "\n",
        "* `recall_score`: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m95539uQZaDZ"
      },
      "source": [
        "# Model Selection - Train & Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Getting Metrics: Accuracy - Precision & Recall Score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "data_train, data_test, classes_train, classes_test = train_test_split(data, classes, test_size=0.10, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qrb-42APpHM"
      },
      "source": [
        "# Naive Bayes Classifier Invokation\n",
        "classifier = NaiveBayesClassifier()\n",
        "# Applying NB on Models\n",
        "classifier.fit(data_train, classes_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdzx7-5xXJfD"
      },
      "source": [
        "# Checking In Predictions\n",
        "clases_predict = classifier.predict(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0whDaiFwayf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b32fd2-1963-48e3-a9a2-969aa4b839c2"
      },
      "source": [
        "# Gettign Accuracy Score\n",
        "accuracy_score(classes_test, clases_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8397683397683398"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQM3QjN1dlIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7689c828-09e4-4297-b0b5-d621cbfdc652"
      },
      "source": [
        "# Precision Score\n",
        "precision_score(classes_test, clases_predict, average=None, zero_division=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81390135, 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiWQMX0Jdmnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b04e9c5-18b9-49a4-a4e6-9f28323ec18c"
      },
      "source": [
        "# Recall Score\n",
        "recall_score(classes_test, clases_predict, average=None, zero_division=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.46451613])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}